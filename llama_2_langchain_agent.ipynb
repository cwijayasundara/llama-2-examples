{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLaMa 7B Chatbot in Hugging Face and LangChain\n",
        "\n",
        "Trying to explore how we can use the open source **Llama-7b-chat** model in both Hugging Face transformers and LangChain.\n",
        "You must first request access to Llama 2 models via [this form]() (access is typically granted within a few hours)."
      ],
      "metadata": {
        "id": "OW9SKqaV0Qjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that running this on CPU is practically impossible. It will take a very long time. If running on Google Colab you go to **Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > A100 or V100**."
      ],
      "metadata": {
        "id": "4R-vA-2L0vCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhWUpBHk0Drv"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers==4.31.0 \\\n",
        "    accelerate==0.21.0 \\\n",
        "    einops==0.6.1 \\\n",
        "    langchain==0.0.240 \\\n",
        "    xformers==0.0.20 \\\n",
        "    bitsandbytes==0.41.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to initialize a text-generation pipeline with Hugging Face transformers. The Pipeline requires three things that we must initialize first, those are:\n",
        "\n",
        "- A LLM, in this case it will be meta-llama/Llama-2-7b-chat-hf.\n",
        "- The respective tokenizer for the model.\n",
        "\n",
        "We initialize the model and move it to our CUDA-enabled GPU. Using Colab this can take 5-10 minutes to download and initialize the model."
      ],
      "metadata": {
        "id": "61vq3heb2Er9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory which requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '<hf-token>'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "npcMI-9c2Dox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 7B models were trained using the Llama 2 7B tokenizer, which we initialize like so:"
      ],
      "metadata": {
        "id": "-NxYTjrW22EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "oT569PZn3AyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code."
      ],
      "metadata": {
        "id": "8BtuzhlF3mjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "RGRLpea83gWO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the meta-llama/Llama-2-7b-chat-hf via HuggingFace pipeline"
      ],
      "metadata": {
        "id": "xDjUHCwR3Fyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm(prompt=\"Explain to me the difference between nuclear fission and fusion.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "5w_DC2Pf3We7",
        "outputId": "128b197f-cc61-4d19-8857-4e5a3aa26338"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Unterscheidung between nuclear fission and fusion: Nuclear fission is a process in which an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. Nuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus.\\nNuclear fission is a process where an atomic nucleus splits into two or more smaller nuclei, releasing energy in the process. This process typically occurs when an atom's nucleus is bombarded with a high-energy particle, such as a neutron. When this happens, the nucleus can become unstable and break apart into lighter elements, releasing a large amount of energy in the process.\\nNuclear fusion, on the other hand, is the process by which two or more atomic nuclei combine to form a single, heavier nucleus. This process also releases a large amount of energy, but it is much more difficult to achieve than nuclear fission because the nuclei being fused must be brought together at very high temperatures and pressures.\\nOne key difference between nuclear fission and fusion is the type of particles involved. In nuclear fission, the particles are typically protons or neutrons, while in nuclear fusion, the particles are typically electrons or protons. Another difference is the direction of the energy release. In nuclear fission, the energy is released in the form of kinetic energy of the fragments, while in nuclear fusion, the energy is released in the form of electromagnetic radiation.\\nAnother important difference between nuclear fission and fusion is their potential applications. Nuclear fission is the primary source of nuclear power used to generate electricity in many countries, while nuclear fusion is still in the experimental stage and has not yet been harnessed for practical applications. However, some researchers believe that nuclear fusion could potentially provide a nearly limitless source of clean energy in the future.\\nIn summary, nuclear fission and fusion are two different processes involving the nucleus of an atom. While they both release energy, they differ in the type of particles involved, the direction of energy release, and their potential applications.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(prompt=\"what is 4 to the power of 2.1?\")"
      ],
      "metadata": {
        "id": "8c1d1vG1_Bj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see if we can get a Langchain conversational agent working with meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "We first need to initialize the agent. Conversational agents require several things such as conversational `memory`, access to `tools`, and an `llm` (which we have already initialized)."
      ],
      "metadata": {
        "id": "F6PKfWBv4571"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.agents import load_tools\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
        ")\n",
        "tools = load_tools([\"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "hh7rrQKn5Aem"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mDz5IUK15jYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentOutputParser\n",
        "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
        "from langchain.output_parsers.json import parse_json_markdown\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "class OutputParser(AgentOutputParser):\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return FORMAT_INSTRUCTIONS\n",
        "\n",
        "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
        "        try:\n",
        "            # this will work IF the text is a valid JSON with action and action_input\n",
        "            response = parse_json_markdown(text)\n",
        "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
        "            if action == \"Final Answer\":\n",
        "                # this means the agent is finished so we call AgentFinish\n",
        "                return AgentFinish({\"output\": action_input}, text)\n",
        "            else:\n",
        "                # otherwise the agent wants to use an action, so we call AgentAction\n",
        "                return AgentAction(action, action_input, text)\n",
        "        except Exception:\n",
        "            # sometimes the agent will return a string that is not a valid JSON\n",
        "            # often this happens when the agent is finished\n",
        "            # so we just return the text as the output\n",
        "            return AgentFinish({\"output\": text}, text)\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"conversational_chat\"\n",
        "\n",
        "# initialize output parser for agent\n",
        "parser = OutputParser()"
      ],
      "metadata": {
        "id": "1Vp0-pR75kB2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    agent_kwargs={\"output_parser\": parser}\n",
        ")"
      ],
      "metadata": {
        "id": "BAUIc5cE6bLl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.agent.llm_chain.prompt"
      ],
      "metadata": {
        "id": "XKbmn6i-6n1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to add special tokens used to signify the beginning and end of instructions, and the beginning and end of system messages. These are described in the Llama-2 model cards on Hugging Face."
      ],
      "metadata": {
        "id": "UY2PwpiF7-D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\""
      ],
      "metadata": {
        "id": "xhcqBOmk77RU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_msg = B_SYS + \"\"\"\n",
        "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
        "All of Assistant's communication is performed using JSON format.\n",
        "\n",
        "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
        "\n",
        "- \"Calculator\": Useful for when you need to answer questions about math.\n",
        "  - To use the calculator tool, Assistant should write like so:\n",
        "    ```json\n",
        "    {{\"action\": \"Calculator\",\n",
        "      \"action_input\": \"sqrt(4)\"}}\n",
        "    ```\n",
        "\"\"\" + E_SYS\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=sys_msg,\n",
        "    tools=tools\n",
        ")\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ],
      "metadata": {
        "id": "u2vw79Cp79Ft"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = B_INST + \" Respond to the following in JSON with 'action' and 'action_input' values \" + E_INST\n",
        "human_msg = instruction + \"\\nUser: {input}\"\n",
        "\n",
        "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
      ],
      "metadata": {
        "id": "pymlZxp78010"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"Explain to me the difference between nuclear fission and fusion.\")"
      ],
      "metadata": {
        "id": "6llDjb3F661W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}